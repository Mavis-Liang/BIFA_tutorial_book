[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Tutorial for Bayesian Integrative Factor Models",
    "section": "",
    "text": "Welcome\nThis is the tutorial to guide statisticians to use Baysian integrative factor models.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "Preliminary.html",
    "href": "Preliminary.html",
    "title": "1  Preliminary",
    "section": "",
    "text": "1.1 Factor models and multi-study setting\nThis chapter introduce the theory behind factor models\nSee Knuth (1984) for additional discussion of literate programming.\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preliminary</span>"
    ]
  },
  {
    "objectID": "Preliminary.html#factor-models-and-multi-study-setting",
    "href": "Preliminary.html#factor-models-and-multi-study-setting",
    "title": "1  Preliminary",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preliminary</span>"
    ]
  },
  {
    "objectID": "quick_start.html",
    "href": "quick_start.html",
    "title": "2  Quick start",
    "section": "",
    "text": "Step 1: Prepare the package and data\nA small simulated data can be downloaded here:\nRDS format\nStep 2: Run BMSFA\nStep 3: Post-processing\nStep 4: Visualization\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quick start</span>"
    ]
  },
  {
    "objectID": "Installation.html",
    "href": "Installation.html",
    "title": "3  Package Installation",
    "section": "",
    "text": "3.1 Stack FA, Ind FA, and BMSFA",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Package Installation</span>"
    ]
  },
  {
    "objectID": "Installation.html#pfa",
    "href": "Installation.html#pfa",
    "title": "3  Package Installation",
    "section": "3.2 PFA",
    "text": "3.2 PFA\nPFA does not provide any downloadable R packages and we need to download the R scripts from their GitHub repository, put them in the same directory as the main script, and source them for use.\nWe only need the three files: FBPFA-PFA.R, FBPFA-PFA with fixed latent dim.R, and PFA.cpp, which can be found in . The FBPFA-PFA.R file contains the full Bayesian inference algorithm for the PFA model, directly set the latent dimensions equal to the dimensions or the original data. The FBPFA-PFA with fixed latent dim.R file contains the same algorithm that requires to set numbers of common factors \\(K\\). We also notice that two version of the models are both PFA(), and some functions in the FBPFA-PFA with fixed latent dim.R file depends on the FBPFA-PFA.R file. Therefore, since we want to run the dimension reduction version of the model, we must source the FBPFA-PFA.R file first, and then source the FBPFA-PFA with fixed latent dim.R file.\n\n# Suppose the files are in the same directory as the main script\nsource(\"FBPFA-PFA.R\")\nsource(\"FBPFA-PFA with fixed latent dim.R\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Package Installation</span>"
    ]
  },
  {
    "objectID": "Installation.html#mom-ss",
    "href": "Installation.html#mom-ss",
    "title": "3  Package Installation",
    "section": "3.3 MOM-SS",
    "text": "3.3 MOM-SS",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Package Installation</span>"
    ]
  },
  {
    "objectID": "Installation.html#sufa",
    "href": "Installation.html#sufa",
    "title": "3  Package Installation",
    "section": "3.4 SUFA",
    "text": "3.4 SUFA",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Package Installation</span>"
    ]
  },
  {
    "objectID": "Installation.html#tetris",
    "href": "Installation.html#tetris",
    "title": "3  Package Installation",
    "section": "3.5 Tetris",
    "text": "3.5 Tetris",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Package Installation</span>"
    ]
  },
  {
    "objectID": "Installation.html#other-utility-packages",
    "href": "Installation.html#other-utility-packages",
    "title": "3  Package Installation",
    "section": "3.6 Other utility packages",
    "text": "3.6 Other utility packages\n\nlibrary(tidyverse)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Package Installation</span>"
    ]
  },
  {
    "objectID": "Nutrition.html",
    "href": "Nutrition.html",
    "title": "4  Case study: nutrition data",
    "section": "",
    "text": "4.1 Loading and previewing the data\nThe data used in this section is from… This data is not publicly available. Please contact the authors of the original study for access.\nload(\"./Data/dataLAT_projale2.rda\")\nThe resulting object is a list of 6 data frames, each corresponding to a different study. Each data frame contains information about the nutritional intake of individuals, and the columns represent different nutrients. From Study 1 to Study 6, the number of individuals (\\(N_s\\)) are 1364, 1517, 2210, 5184, 2478, and 959, respectively, and the number of nutrients (\\(P\\)) are all 42.\n# Check how many studies in the list\nlength(X_s2)\n\n[1] 6\n\n# Dimension of each study\nlapply(X_s2, dim)\n\n[[1]]\n[1] 1364   42\n\n[[2]]\n[1] 1517   42\n\n[[3]]\n[1] 2210   42\n\n[[4]]\n[1] 5184   42\n\n[[5]]\n[1] 2478   42\n\n[[6]]\n[1] 959  42\nLet’s take a look at the first few rows of the first data frame to get an idea of the data structure.\nX_s2[[1]][1:5, 1:5]\n\n  Animal Protein (g) Vegetable Protein (g) Cholesterol (mg)  SCSFA MCSFA\n1            28.9560               14.7440          256.761 0.2665 0.939\n2            33.6675                8.9710          104.217 0.2180 0.520\n3            70.0000               31.0635          207.902 0.9845 1.692\n4            20.6700               13.8240          148.921 0.0625 0.239\n5            15.4250               10.5550           65.060 0.0090 0.033\nWe note that the data we have available is different from the original data (cite). The original data is a collection of 12 studies, and there are known covariates for each individuals, like the one we simulated in the previous section. However, for the purpose of this case study, the data we used are collapsed into 6 studies, and only the nutritional intake data are available.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Case study: nutrition data</span>"
    ]
  },
  {
    "objectID": "Nutrition.html#data-preprocessing",
    "href": "Nutrition.html#data-preprocessing",
    "title": "4  Case study: nutrition data",
    "section": "4.2 Data preprocessing",
    "text": "4.2 Data preprocessing\nSome individuals have missing values for all nutrients, thus we will remove these individuals from the data. Also, there are some nutrition intake are less than zero, for which we will replace with 0. We then apply a log transformation to the data.\nWe first count how many NA values and negative values are in each study.\n\ncount_na_and_negatives &lt;- function(df) {\n  # Count NA values\n  na_count &lt;- sum(is.na(df))\n  # Count negative values\n  negative_count &lt;- sum(df &lt; 0, na.rm = TRUE)\n  \n  # Print counts\n  cat(\"Number of NAs:\", na_count, \"\\n\")\n  cat(\"Number of negative values:\", negative_count, \"\\n\")\n}\ninvisible(lapply(X_s2, count_na_and_negatives))\n\nNumber of NAs: 1344 \nNumber of negative values: 0 \nNumber of NAs: 1344 \nNumber of negative values: 1 \nNumber of NAs: 1344 \nNumber of negative values: 0 \nNumber of NAs: 1344 \nNumber of negative values: 2 \nNumber of NAs: 1344 \nNumber of negative values: 1 \nNumber of NAs: 1344 \nNumber of negative values: 0 \n\n\nWe will define a function to process the data, which removes rows where all values are NA. We also define a function that replaces negative values with 0, and applies a log transformation to the data.\n\nprocess_study_data &lt;- function(df) {\n  # Remove rows where all values are NA\n  cleaned_df &lt;- df[!apply(df, 1, function(row) all(is.na(row))), , drop = FALSE]\n  # Count remaining rows\n  remaining_rows &lt;- nrow(cleaned_df)\n  # Print results for the study\n  cat(\"Remaining rows:\", remaining_rows, \"\\n\")\n  return(cleaned_df)\n}\nY_list &lt;- lapply(X_s2, process_study_data)\n\nRemaining rows: 1332 \nRemaining rows: 1485 \nRemaining rows: 2178 \nRemaining rows: 5152 \nRemaining rows: 2446 \nRemaining rows: 927 \n\n\nThe numbers of individuals in each study left for analysis (\\(N_s\\)) are 1332, 1485, 2178, 5152, 2446, and 927, respectively.\n\n# Replace negative values with 0, then log(x+0.01) + 0.01\nreplace_negatives &lt;- function(df) {\n  # Replace negative values with 0\n  df[df &lt; 0] &lt;- 0\n  # Apply log transformation. Add 0.01 to avoid log(0).\n  transformed_df &lt;- log(df + 0.01)\n  return(transformed_df)\n}\n\nY_list &lt;- lapply(Y_list, replace_negatives)\n\n\n# Check the processed data\ninvisible(lapply(Y_list, count_na_and_negatives))\n\nNumber of NAs: 0 \nNumber of negative values: 11910 \nNumber of NAs: 0 \nNumber of negative values: 11222 \nNumber of NAs: 0 \nNumber of negative values: 15006 \nNumber of NAs: 0 \nNumber of negative values: 36230 \nNumber of NAs: 0 \nNumber of negative values: 19349 \nNumber of NAs: 0 \nNumber of negative values: 6707 \n\n\nNow we don’t have any NA values or negative values in the data.\nThe assumptions for factor models require that each variable has a mean of 0. Therefore, for each study, we will center the data for each column. We note that for some model (Stack FA, Ind FA, BMSFA, and Tetris), this step is handeled internally, and for MOM-SS, the random intercepts are estimated, so we do not need to center the data.\n\nY_list_scaled &lt;- lapply(\n  Y_list, function(x) scale(x, center = TRUE, scale = FALSE)\n)\nY_mat_scaled &lt;- Y_list_scaled %&gt;% do.call(rbind, .) %&gt;% as.matrix()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Case study: nutrition data</span>"
    ]
  },
  {
    "objectID": "Nutrition.html#model-fitting",
    "href": "Nutrition.html#model-fitting",
    "title": "4  Case study: nutrition data",
    "section": "4.3 Model fitting",
    "text": "4.3 Model fitting\nWe recommend that we run the following code chunk in a high-performance computing environment, as the model fitting process can be computationally intensive. PFA and Tetris are particularly computationally expensive, where PFA requires more than 10 hours to run, and Tetris requires more than 4 days to run. Other models can be finished within half an hour. We recommend at least 5GB of memory for running the models and post-processing.\nFor each model, we over-specify the numbers of factors to .. [to be written].\n\n# Stack FA\nY_mat =  Y_list %&gt;% do.call(rbind, .) %&gt;% as.matrix()\nfit_stackFA &lt;- MSFA::sp_fa(Y_mat, k = 6, scaling = FALSE, centering = TRUE, \n                              control = list(nrun = 10000, burn = 800))\n# Ind FA\nfit_indFA &lt;-\n      lapply(1:6, function(s){\n        j_s = c(8, 8, 8, 8, 8, 8)\n        MSFA::sp_fa(Y_list[[s]], k = j_s[s], scaling = FALSE, centering = TRUE,\n                    control = list(nrun = 10000, burn = 8000))\n      })\n\n# PFA\nN_s &lt;- sapply(Y_list, nrow)\nfit_PFA &lt;- PFA(Y=t(Y_mat_scaled),\n                        latentdim = 6,\n                        grpind = rep(1:6,\n                                    times = N_s),\n                Thin = 5,\n                Total_itr = 10000, burn = 8000)\n\n# MOM-SS\nY_mat =  Y_list %&gt;% do.call(rbind, .) %&gt;% as.matrix()\n# Construct the membership matrix\nN_s &lt;- sapply(Y_list, nrow)\nM_list &lt;- list()\n   for(s in 1:6){\n     M_list[[s]] &lt;- matrix(1, nrow = N_s[s], ncol = 1)\n   }\nM &lt;- as.matrix(bdiag(M_list))\nfit_MOMSS &lt;- BFR.BE::BFR.BE.EM.CV(x = Y_mat, v = NULL, \n                                  b = M, q = 6, scaling = FALSE)\n\n\n# SUFA\nfit_SUFA &lt;- SUFA::fit_SUFA(Y_list_scaled, qmax=6, nrun = 10000)\n\n# BMSFA\nfit_BMSFA &lt;- MSFA::sp_msfa(Y_list, k = 6, j_s = c(2, 2, 2, 2, 2, 2),\n                           outputlevel = 1, scaling = FALSE, \n                           centering = TRUE,\n                           control = list(nrun = 10000, burn = 8000))\n\nFitting Tetris requires a 3-step process. First, we run tetris() to draw posterior samples of the model parameters, including \\(\\mathcal{T}\\). Then we run choose.A() to choose the best \\(\\mathcal{T}\\) based on the posterior samples. Finally, we run tetris() again with the chosen \\(\\mathcal{T}\\) to obtain the final model. Hyperparamters \\(\\alpha_{\\mathcal{T}}\\) are set to 1.25 times the number of studies.\n\n# Tetris\nset_alpha &lt;- ceiling(1.25*6)\nfit_Tetris &lt;- tetris(Y_list, alpha=set_alpha, beta=1, nprint = 200, \n                     nrun=10000, burn=8000)\nbig_T &lt;- choose.A(fit_Tetris, alpha_IBP=set_alpha, S=6)\nrun_fixed &lt;- tetris(Y_list, alpha=set_alpha, beta=1, \n                    fixed=TRUE, A_fixed=big_T, nprint = 200, \n                    nrun=10000, burn=8000)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Case study: nutrition data</span>"
    ]
  },
  {
    "objectID": "Nutrition.html#post-processing",
    "href": "Nutrition.html#post-processing",
    "title": "4  Case study: nutrition data",
    "section": "4.4 Post processing",
    "text": "4.4 Post processing\nPost processing includes calculating the point estimates of the factor loadings and covariance matrix from the posterior samples, and determing the number of factors for each model.\nFor methods of MOM-SS, SUFA, and Tetris, the number of factors is determined internally in the algorithms, therefore, the output of the models are final results.\nFor MOM-SS, \\(\\Phi\\) is directly obtained with its post-processed common loadings in the fitted output. Th common covariance is calculated with \\(\\Phi\\Phi^\\top\\). The marginal covariance matrix \\(\\Sigma_{\\text{marginal}}\\) is calculated by adding the estimated study-specific error covariances to the common variance. The study-specific intercepts \\(\\alpha\\) and the coefficients for the known covariates \\(B\\) are also extracted from the fitted output.\n\npost_MOMSS &lt;- function(fit, version = 2){ # version 1: M, version 2: Mpost\n  est_Phi &lt;- fit$M\n  if (version==2){est_Phi &lt;- fit$Mpost}\n  est_SigmaPhi &lt;- tcrossprod(est_Phi)\n  \n  # Marginal covariance\n  S &lt;- dim(fit$sigma)[2]\n  est_PsiList &lt;- est_SigmaMarginal &lt;-  list()\n  for(s in 1:S){\n    est_PsiList[[s]] &lt;- fit$sigma[,s]\n    est_SigmaMarginal[[s]] &lt;- est_SigmaPhi + diag(fit$sigma[,s])\n  }\n  # last S columns of fit$Theta are the study-specific intercepts\n  est_alphas &lt;- fit$Theta[, (dim(fit$Theta)[2]-S+1):dim(fit$Theta)[2]]\n  # The rest are coeficients for the known covariates\n  est_B &lt;- fit$Theta[, 1:(dim(fit$Theta)[2]-S)]\n  \n  return(list(Phi = est_Phi, SigmaPhi = est_SigmaPhi, Psi = est_PsiList, alpha = est_alphas, B = est_B,\n              SigmaMarginal = est_SigmaMarginal))\n}\n\nFor SUFA, the shared and study-specific loading matrices, as well as the common and marginal covariance can be conveniently obtained via the lam.est.all(), SUFA_shared_covmat() and sufa_marginal_covs() functions. Error covariance is obtained by taking averages of the “residuals” fitted output. And the study-specific covariance matrices are calculated by subtracting the common covariance from the marginal covariance. Note that in the definition of SUFA, common covariance is \\(\\Phi\\Phi^\\top + \\Sigma\\).\n\npost_SUFA &lt;- function(fit){\n  all &lt;- dim(fit$Lambda)[3]\n  burnin &lt;- floor(all * 0.8) # We will use the last 20% samples\n  # shared and study-specific loading matrices\n  loadings &lt;- lam.est.all(fit, burn = burnin)\n  # Obtain common covariance matrix and loading from fitting\n  est_Phi &lt;- loadings$Shared\n  est_SigmaPhi &lt;- SUFA_shared_covmat(fit, burn = burnin)\n  est_Psi &lt;- diag(colMeans(fit$residuals))\n  # Study-specific loadings\n  est_LambdaList &lt;- loadings$Study_specific\n  \n  # Obtain study-specific covariance matrices\n  S &lt;- length(fit$A)\n  marginal_cov &lt;- sufa_marginal_covs(fit, burn = burnin)\n  est_SigmaLambdaList &lt;- list()\n  for (s in 1:S) {\n    est_SigmaLambdaList[[s]] &lt;- marginal_cov[,,s] - est_SigmaPhi\n  }\n  \n  return(list(SigmaPhi = est_SigmaPhi, Phi = est_Phi, \n              SigmaLambdaList = est_SigmaLambdaList,\n              LambdaList = est_LambdaList, \n              Psi = est_Psi,\n              SigmaMarginal = lapply(1:S, function(s) marginal_cov[,,s])\n              ))\n}\n\nFor Tetris, the common loading matrix \\(\\Phi\\) can be obtained through the getLambda() function. The common covariance matrix is calculated as \\(\\Phi\\Phi^\\top\\). The study-specific loading matrices \\(\\Lambda_s\\) are obtained by multiplying the common loading matrix with the study-specific matrices \\(T_s - P\\). The study-specific covariance matrices are calculated as \\(\\Lambda_s\\Lambda_s^\\top\\). The marginal covariance matrix is calculated as \\(\\Lambda T \\Lambda^\\top + \\Psi\\).\n\npost_Tetris &lt;- function(fit){\n  # Estimated common covariance\n  A &lt;- fit$A[[1]]\n  Lambda &lt;- getLambda(fit,A)\n  S &lt;- dim(A)[1]\n  est_Phi &lt;- as.matrix(Lambda[,colSums(A)==S])\n  est_SigmaPhi &lt;- tcrossprod(est_Phi)\n  # Estimated study-specific covariance\n  P = diag((colSums(A) == S)*1)\n  T_s &lt;- list()\n  est_LambdaList &lt;- list()\n  for(s in 1:S){\n    T_s[[s]] &lt;- diag(A[s,])\n    Lambda_s &lt;- Lambda %*% (T_s[[s]] - P)\n    Lambda_s &lt;- Lambda_s[,-which(colSums(Lambda_s == 0) == nrow(Lambda_s))]\n    Lambda_s &lt;- matrix(Lambda_s, nrow=nrow(Lambda))\n    est_LambdaList[[s]] &lt;- Lambda_s}\n  est_SigmaLambdaList &lt;- lapply(1:S, function(s){\n    tcrossprod(est_LambdaList[[s]])})\n  \n  # Estimated marginal covariance\n  Psi &lt;- list()\n  est_SigmaMarginal &lt;- lapply(1:S, function(s){\n    Psi[[s]] &lt;- diag(Reduce(\"+\", fit$Psi[[s]])/length(fit$Psi[[s]]))\n    Sigma_s &lt;- Lambda %*% T_s[[s]] %*% t(Lambda) + Psi[[s]]\n    })\n  \n  return(list(Phi = est_Phi, SigmaPhi = est_SigmaPhi,\n              LambdaList = est_LambdaList, SigmaLambdaList = est_SigmaLambdaList,\n              Psi = Psi, T_s = T_s,\n              SigmaMarginal = est_SigmaMarginal))\n}\n\nFor PFA, the post-processing is a little bit tricky. According to the original paper, the common loading matrix \\(\\Phi\\) can be just the average of the estimated loadings from the posterior samples. However, we still apply OP as in BMSFA to get the common loadings, and the loadings in PFA should be the product of the estimated loadings and the square root of the variance of the factors. Therefore, we carry out multiplication for \\(\\Phi\\) for each posterior sample, and then OP the sequence. We also carry out multiplication for \\(Q_s\\), \\(\\Sigma_{\\Phi}\\), \\(\\Sigma_{\\text{marginal}}\\), \\(\\Sigma_{\\Lambda_s}\\), and \\(\\Psi\\) as in the definition of PFA, and then take the average of the results.\nAfter that, the number of factors is determined by counting the number of columns in \\(\\Phi\\) that have all loadings less than \\(10^{-3}\\).\n\npost_PFA &lt;- function(fit){\n  p &lt;- nrow(fit$Loading[[1]])\n  k &lt;- ncol(fit$Loading[[1]])\n  npost &lt;- length(fit$Loading)\n  Q_list &lt;- fit$Pertmat\n  S &lt;- dim(Q_list[[1]])[2]\n  posteriorPhis &lt;- array(0, dim = c(p, k, npost))\n  \n  #---estimated common loadings and shared variance---\n  # Element-wise multiplication\n  for(i in 1:npost){\n    posteriorPhis[,,i] &lt;- fit$Loading[[i]] %*% diag(fit$Latentsigma[[i]])\n  }\n  # Varimax for common loadings\n  est_Phi &lt;- MSFA::sp_OP(posteriorPhis, itermax = 10, trace = FALSE)$Phi\n  \n  # Estimated shared covariance, study-specific covariance matrix, and SigmaMarginal\n  sharevar &lt;- list()\n  est_SigmaLambdaList &lt;- list()\n  est_SigmaMarginal &lt;- list()\n  est_Psi &lt;- list()\n  for(s in 1:S){ # Loop over each study\n  post_SigmaLambda_s &lt;- list()\n  post_SigmaMarginal_s &lt;- list()\n  Psi &lt;- list()\n  for(i in 1:npost){ # Loop over each posterior sample\n    sharevar[[i]] &lt;- fit$Loading[[i]]%*%diag(fit$Latentsigma[[i]]^2)%*%t(fit$Loading[[i]]) + \n      diag(fit$Errorsigma[[i]]^2) # Get the shared variance\n    Q_temp_inv &lt;- solve(\n      matrix(Q_list[[i]][, s], p, p)\n      )\n    post_SigmaMarginal_s[[i]] &lt;- Q_temp_inv%*%sharevar[[i]]%*%t(Q_temp_inv)\n    post_SigmaLambda_s[[i]] &lt;- post_SigmaMarginal_s[[i]] - sharevar[[i]]\n    Psi[[i]] &lt;- diag(fit$Errorsigma[[i]]^2)\n  }\n  est_SigmaMarginal[[s]] &lt;- Reduce('+', post_SigmaMarginal_s)/length(post_SigmaMarginal_s)\n  est_SigmaLambdaList[[s]] &lt;- Reduce('+', post_SigmaLambda_s)/length(post_SigmaLambda_s)\n  }\n  est_Psi &lt;- Reduce('+', Psi)/length(Psi)\n  est_SigmaPhi &lt;- Reduce('+', sharevar)/length(sharevar)\n  est_Q &lt;- Reduce('+', Q_list)/length(Q_list)\n  est_Q_list &lt;- lapply(1:S, function(s) matrix(est_Q[, s], p, p))\n  \n  # Return the results\n  return(list(Phi = est_Phi, SigmaPhi = est_SigmaPhi, Psi = est_Psi, Q = est_Q_list,\n              SigmaLambdaList = est_SigmaLambdaList,\n              SigmaMarginal = est_SigmaMarginal))\n}\n\n\n# columns have all loadings less than 10^-3\nfun_neibour &lt;- function(Phi, threshold = 1e-3) {\n  return(\n    sum(apply(Phi, 2, function(x) {\n      sum(abs(x) &lt;= threshold) &lt; length(x)}\n    ))\n  )\n}\nPhi_PFA &lt;- readRDS(\"Data/Rnutrition_PFA.rds\")$Phi # Load post_PFA(fit_PFA) output\nK_PFA &lt;- fun_neibour(Phi_PFA)\nK_PFA\n\n[1] 6",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Case study: nutrition data</span>"
    ]
  },
  {
    "objectID": "Nutrition.html#visualization",
    "href": "Nutrition.html#visualization",
    "title": "4  Case study: nutrition data",
    "section": "4.5 Visualization",
    "text": "4.5 Visualization",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Case study: nutrition data</span>"
    ]
  },
  {
    "objectID": "Nutrition.html#mean-squared-error-mse",
    "href": "Nutrition.html#mean-squared-error-mse",
    "title": "4  Case study: nutrition data",
    "section": "4.6 Mean squared error (MSE)",
    "text": "4.6 Mean squared error (MSE)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Case study: nutrition data</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "5  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  }
]